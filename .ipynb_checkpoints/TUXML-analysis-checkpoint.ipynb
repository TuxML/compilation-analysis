{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "combined_pd = pd.read_pickle('config_bdd30-100.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert (len(combined_pd) == 20001)\n",
    "len(combined_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_head = [\"cid\", \"time\", \"date\"] # \"compile\"\n",
    "size_methods = [\"vmlinux\", \"GZIP-bzImage\", \"GZIP-vmlinux\", \"GZIP\", \"BZIP2-bzImage\", \n",
    "              \"BZIP2-vmlinux\", \"BZIP2\", \"LZMA-bzImage\", \"LZMA-vmlinux\", \"LZMA\", \"XZ-bzImage\", \"XZ-vmlinux\", \"XZ\", \n",
    "              \"LZO-bzImage\", \"LZO-vmlinux\", \"LZO\", \"LZ4-bzImage\", \"LZ4-vmlinux\", \"LZ4\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### basic stats about options and unique values \n",
    "## could be extended/improved \n",
    "\n",
    "tri_state_values = ['y', 'n', 'm']\n",
    "\n",
    "ftuniques = []\n",
    "freq_ymn_features = []\n",
    "non_tristate_options = []\n",
    "\n",
    "for col in combined_pd:\n",
    "    ft = combined_pd[col]    \n",
    "    # eg always \"y\"\n",
    "    if len(ft.unique()) == 1:\n",
    "        ftuniques.append(col)\n",
    "    # only tri-state values (y, n, m) (possible TODO: handle numerical/string options)    \n",
    "    elif all(x in tri_state_values for x in ft.unique()):     #len(ft.unique()) == 3: \n",
    "        freq = ft.value_counts(normalize=True)\n",
    "        freqy = 0\n",
    "        freqn = 0\n",
    "        freqm = 0\n",
    "        if ('y' in freq.index):\n",
    "            freqy = freq['y']\n",
    "        if ('n' in freq.index):\n",
    "            freqn = freq['n']\n",
    "        if ('m' in freq.index):\n",
    "            freqm = freq['m']\n",
    "        freq_ymn_features.append((col, freqy, freqm, freqn))\n",
    "    else:\n",
    "        if not (col in size_methods): \n",
    "            non_tristate_options.append(col)\n",
    "        \n",
    "\n",
    "### TODO: we want to keep all quantitative values!\n",
    "# non_tristate_options.remove('LZO') # ('vmlinux')\n",
    "\n",
    "# we want to keep measurements (that are not tristate ;)) \n",
    "# non_tristate_options = list(set(non_tristate_options) - set(size_methods))\n",
    "\n",
    "#### print options with unique values\n",
    "# options with only one value eg always \"y\"\n",
    "#i = 0\n",
    "#for ft in ftuniques:\n",
    "#    print(ft + \" (\" + str(i) + \")\")\n",
    "#    i = i + 1\n",
    "\n",
    "print(\"Original size (#configs/#options) of the dataset \" + str(combined_pd.shape))\n",
    "print (\"Number of options with only one value (eg always y): \" + str(pd.DataFrame(ftuniques).shape))\n",
    "\n",
    "# maybe we can drop options with only one unique value (no interest for machine learning)\n",
    "# TODO: maybe we can rely on more traditional feature reduction techniques\n",
    "# TODO: need to think about *when* to apply the removal \n",
    "\n",
    "#rawtuxdata.drop(columns=ftuniques,inplace=True) \n",
    "\n",
    "## non_tristate_options include basic stuff like date, time, cid but also string/numerical options\n",
    "print (\"Non tri-state value options (eg string or integer or hybrid values): \" \n",
    "       + str(pd.DataFrame(non_tristate_options).shape) + \" \") \n",
    "#      + str(pd.DataFrame(non_tristate_options)))\n",
    "\n",
    "\n",
    "print (\"Predictor variables: \" + str(combined_pd.drop(columns=non_tristate_options).columns.size))\n",
    "# frequency of y, m, and n values \n",
    "#plt.figure()\n",
    "#pd.DataFrame(freq_ymn_features, columns=[\"feature\", \"freqy\", \"freqm\", \"freqn\"]).plot(kind='hist', alpha=0.8) #plot()\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "combined_pd.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import *\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor, GradientBoostingClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn import tree\n",
    "import graphviz \n",
    "\n",
    "\n",
    "LEARN_COMPILATION_SUCCESS = True # costly in time and space \n",
    "compilation_status_column_name = 'compile_success'\n",
    "\n",
    "def encode_data_compilation(rawtuxdata):\n",
    "    lae = LabelEncoder()\n",
    "    # we save quantitative values we want (here vmlinux, TODO: generalize)\n",
    "    # the key idea is that the labelling encoder should not be applied to this kind of values (only to predictor variables!)\n",
    "    # vml = rawtuxdata['LZO'] # rawtuxdata['vmlinux'] \n",
    "    o_sizes = rawtuxdata[size_methods]\n",
    "\n",
    "    # we may remove non tri state options, but TODO there are perhaps some interesting options (numerical or string) here\n",
    "    #tuxdata = rawtuxdata.drop(columns=non_tristate_options).drop(columns=['vmlinux']).apply(le.fit_transform)\n",
    "    tuxdata_for_compilation = rawtuxdata.drop(columns=non_tristate_options).drop(columns=size_methods).apply(lae.fit_transform)\n",
    "\n",
    "    #tuxdata['vmlinux'] = vml \n",
    "    tuxdata_for_compilation[size_methods] = o_sizes\n",
    "    # we can ue vmlinux since it has been restored thanks to previous line\n",
    "    tuxdata_for_compilation[compilation_status_column_name] = tuxdata_for_compilation['vmlinux'] != -1\n",
    "    return tuxdata_for_compilation\n",
    "\n",
    "def learn_compilation_success(tuxdata_for_compilation):\n",
    "    TESTING_SIZE=0.1 \n",
    "    X_train, X_test, y_train, y_test = train_test_split(tuxdata_for_compilation.drop(columns=size_methods).drop(columns=compilation_status_column_name), tuxdata_for_compilation[compilation_status_column_name], test_size=TESTING_SIZE, random_state=0)  \n",
    "    clf = tree.DecisionTreeClassifier() #GradientBoostingClassifier(n_estimators=100) #RandomForestRegressor(n_estimators=100) #   #GradientBoostingRegressor(n_estimators=100)  \n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    importances = clf.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]    \n",
    "\n",
    "    TOP_FT_IMPORTANCE=20\n",
    "    print(\"Feature ranking: \" + \"top (\" + str(TOP_FT_IMPORTANCE) + \")\")\n",
    "    for f in range(TOP_FT_IMPORTANCE): # len(indices)\n",
    "        print(\"%d. feature %s %d (%f)\" % (f + 1, tuxdata_for_compilation.columns[indices[f]], indices[f], importances[indices[f]]))\n",
    "   \n",
    "    \n",
    "    dot_data = tree.export_graphviz(clf, out_file=None, \n",
    "                         feature_names=tuxdata_for_compilation.drop(columns=size_methods).drop(columns=compilation_status_column_name).columns,  \n",
    "                         filled=True, rounded=True,\n",
    "                         special_characters=True)  \n",
    "    graph = graphviz.Source(dot_data)  \n",
    "    graph.render(\"TUXML_compilation_failures\")\n",
    "    \n",
    "    acc = accuracy_score (y_test, y_pred)\n",
    "    prec = precision_score (y_test, y_pred)\n",
    "    reca = recall_score (y_test, y_pred)\n",
    "    f1 = f1_score (y_test, y_pred)\n",
    "    print(\"Accuracy score: %.2f\" % (acc))\n",
    "    print(\"Precision score: %.2f\" % (prec))\n",
    "    print(\"Recall score: %.2f\" % (reca))\n",
    "    print(\"F1 score: %.2f\" % (f1))\n",
    "\n",
    "if (LEARN_COMPILATION_SUCCESS):\n",
    "    tuxdata_for_compilation = encode_data_compilation(combined_pd)\n",
    "    tuxdata_for_compilation [compilation_status_column_name].describe() # TODO?\n",
    "    learn_compilation_success(tuxdata_for_compilation)\n",
    "    del tuxdata_for_compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compilation_failures = len(combined_pd.query(\"vmlinux == -1\").index)\n",
    "compilation_successes = len(combined_pd.query(\"vmlinux != -1\").index)\n",
    "n_compilations = len(combined_pd.index)\n",
    "assert(compilation_successes + compilation_failures == n_compilations)\n",
    "compilation_failure_percentage = (compilation_failures / n_compilations) * 100\n",
    "print(\"compilation failures:\", compilation_failures, \"out of\", n_compilations, \"(\", compilation_failure_percentage, \"%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
